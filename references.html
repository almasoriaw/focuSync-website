<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>focuSync - Research References</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <div class="logo-container">
                <img src="assets/logo.png" alt="focuSync Logo" class="logo">
                <h1>focuSync</h1>
            </div>
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="science.html">The Science</a></li>
                    <li><a href="methodology.html">Methodology</a></li>
                    <li><a href="implementation.html">Implementation</a></li>
                    <li><a href="ethics.html">Ethics</a></li>
                    <li><a href="references.html" class="active">References</a></li>
                    <li><a href="thank-you.html">Thank You</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <section class="hero">
        <div class="container">
            <h2>Research References</h2>
            <p>The academic foundation of focuSync</p>
        </div>
    </section>

    <section class="page-content">
        <div class="container">
            <h2>Faces of Focus. Action Units Analysis</h2>
            
            <div class="reference">
                <h3>Babaei, E., Zhou, Q., Srivastava, N., Dingler, T., Newn, J., & Velloso, E. (2020)</h3>
                <p>Faces of Focus: A Study on the Facial Cues of Attentional States. </p>
                <p>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ’20), April 25–30, 2020, Honolulu, HI, USA. ACM. https://doi.org/10.1145/3313831.3376566.</p>
            </div>
            
        
            
            <h2>Eye Movement and Facial Feature Analysis</h2>
            
            
            <div class="reference">
                <h3>Action Units and Attentional States</h3>
                <p>Research correlating specific facial Action Units with engagement and challenge dimensions:</p>
                <ul>
                    <li>Positive correlation between engagement and AU20 (lip stretcher) and AU25 (lips apart)</li>
                    <li>Negative correlation between engagement and gaze angle, pitch, and AU14 (dimpler)</li>
                    <li>Positive correlation between challenge and AU5 (upper lid raiser)</li>
                    <li>Negative correlation between challenge and gaze angle direction, pitch, and AU28 (lip suck)</li>
                </ul>
            </div>
            
            <h2>Computational Models and Implementations</h2>
            
            <div class="reference">
                <h3>ACT-R Cognitive Architecture</h3>
                <p>Modeling of attention processes and cognitive engagement based on the ACT-R framework.</p>
            </div>
            
            <div class="reference">
                <h3>RT-GENE</h3>
                <p>Real-time eye tracking research and implementation methods that informed our gaze estimation approach.</p>
            </div>
            
            <div class="reference">
                <h3>GazeCapture Dataset (MIT CSAIL)</h3>
                <p>Large-scale dataset for gaze estimation that provided benchmarks and validation approaches.</p>
            </div>
            
            <h2>OpenFace References</h2>
            
            <div class="reference">
                <h3>OpenFace 3.0</h3>
                <p>Open-source facial behavior analysis toolkit providing facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.</p>
            </div>
            
            <div class="reference">
                <h3>Facial Action Coding System (FACS)</h3>
                <p>Comprehensive system for categorizing facial expressions that forms the foundation of the Action Units used in our analysis.</p>
            </div>
            
            <h2>Deep Learning and Temporal Modeling</h2>
            
            <div class="reference">
                <h3>LSTM Networks for Temporal Sequence Processing</h3>
                <p>Research on applying Long Short-Term Memory networks to temporal data sequences for classification tasks.</p>
            </div>
            
            <div class="reference">
                <h3>Neural Networks for Human Behavior Analysis</h3>
                <p>Applications of deep learning approaches to human behavior recognition and classification tasks.</p>
            </div>
            
            <h2>Ethics and User Research</h2>
            
            <div class="reference">
                <h3>AI Ethics Frameworks</h3>
                <p>Comprehensive ethics guidelines for facial analysis technologies and attention monitoring systems.</p>
            </div>
            
            <div class="reference">
                <h3>Human-Computer Interaction Research</h3>
                <p>Best practices for non-intrusive monitoring and user-centered design of attention-aware systems.</p>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 focuSync Project. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
